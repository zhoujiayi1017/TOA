{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFLiteモデルをPyTorch用に変換する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFLiteファイルから重みを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=\"../models/tf/FaceMesh.tflite\")\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  0                        input_1 <class 'numpy.float32'> [  1 192 192   3]\n  1                  conv2d/Kernel <class 'numpy.float32'> [16  3  3  3]\n  2                    conv2d/Bias <class 'numpy.float32'> [16]\n  3                         conv2d <class 'numpy.float32'> [ 1 96 96 16]\n  4                  p_re_lu/Alpha <class 'numpy.float32'> [ 1  1 16]\n  5                        p_re_lu <class 'numpy.float32'> [ 1 96 96 16]\n  6        depthwise_conv2d/Kernel <class 'numpy.float32'> [ 1  3  3 16]\n  7          depthwise_conv2d/Bias <class 'numpy.float32'> [16]\n  8               depthwise_conv2d <class 'numpy.float32'> [ 1 96 96 16]\n  9                conv2d_1/Kernel <class 'numpy.float32'> [16  1  1 16]\n 10                  conv2d_1/Bias <class 'numpy.float32'> [16]\n 11                       conv2d_1 <class 'numpy.float32'> [ 1 96 96 16]\n 12                            add <class 'numpy.float32'> [ 1 96 96 16]\n 13                p_re_lu_1/Alpha <class 'numpy.float32'> [ 1  1 16]\n 14                      p_re_lu_1 <class 'numpy.float32'> [ 1 96 96 16]\n 15      depthwise_conv2d_1/Kernel <class 'numpy.float32'> [ 1  3  3 16]\n 16        depthwise_conv2d_1/Bias <class 'numpy.float32'> [16]\n 17             depthwise_conv2d_1 <class 'numpy.float32'> [ 1 96 96 16]\n 18                conv2d_2/Kernel <class 'numpy.float32'> [16  1  1 16]\n 19                  conv2d_2/Bias <class 'numpy.float32'> [16]\n 20                       conv2d_2 <class 'numpy.float32'> [ 1 96 96 16]\n 21                          add_1 <class 'numpy.float32'> [ 1 96 96 16]\n 22                p_re_lu_2/Alpha <class 'numpy.float32'> [ 1  1 16]\n 23                      p_re_lu_2 <class 'numpy.float32'> [ 1 96 96 16]\n 24      depthwise_conv2d_2/Kernel <class 'numpy.float32'> [ 1  3  3 16]\n 25        depthwise_conv2d_2/Bias <class 'numpy.float32'> [16]\n 26             depthwise_conv2d_2 <class 'numpy.float32'> [ 1 48 48 16]\n 27                  max_pooling2d <class 'numpy.float32'> [ 1 48 48 16]\n 28                conv2d_3/Kernel <class 'numpy.float32'> [32  1  1 16]\n 29                  conv2d_3/Bias <class 'numpy.float32'> [32]\n 30                       conv2d_3 <class 'numpy.float32'> [ 1 48 48 32]\n 31       channel_padding/Paddings <class 'numpy.int32'> [4 2]\n 32                channel_padding <class 'numpy.float32'> [ 1 48 48 32]\n 33                          add_2 <class 'numpy.float32'> [ 1 48 48 32]\n 34                p_re_lu_3/Alpha <class 'numpy.float32'> [ 1  1 32]\n 35                      p_re_lu_3 <class 'numpy.float32'> [ 1 48 48 32]\n 36      depthwise_conv2d_3/Kernel <class 'numpy.float32'> [ 1  3  3 32]\n 37        depthwise_conv2d_3/Bias <class 'numpy.float32'> [32]\n 38             depthwise_conv2d_3 <class 'numpy.float32'> [ 1 48 48 32]\n 39                conv2d_4/Kernel <class 'numpy.float32'> [32  1  1 32]\n 40                  conv2d_4/Bias <class 'numpy.float32'> [32]\n 41                       conv2d_4 <class 'numpy.float32'> [ 1 48 48 32]\n 42                          add_3 <class 'numpy.float32'> [ 1 48 48 32]\n 43                p_re_lu_4/Alpha <class 'numpy.float32'> [ 1  1 32]\n 44                      p_re_lu_4 <class 'numpy.float32'> [ 1 48 48 32]\n 45      depthwise_conv2d_4/Kernel <class 'numpy.float32'> [ 1  3  3 32]\n 46        depthwise_conv2d_4/Bias <class 'numpy.float32'> [32]\n 47             depthwise_conv2d_4 <class 'numpy.float32'> [ 1 48 48 32]\n 48                conv2d_5/Kernel <class 'numpy.float32'> [32  1  1 32]\n 49                  conv2d_5/Bias <class 'numpy.float32'> [32]\n 50                       conv2d_5 <class 'numpy.float32'> [ 1 48 48 32]\n 51                          add_4 <class 'numpy.float32'> [ 1 48 48 32]\n 52                p_re_lu_5/Alpha <class 'numpy.float32'> [ 1  1 32]\n 53                      p_re_lu_5 <class 'numpy.float32'> [ 1 48 48 32]\n 54      depthwise_conv2d_5/Kernel <class 'numpy.float32'> [ 1  3  3 32]\n 55        depthwise_conv2d_5/Bias <class 'numpy.float32'> [32]\n 56             depthwise_conv2d_5 <class 'numpy.float32'> [ 1 24 24 32]\n 57                max_pooling2d_1 <class 'numpy.float32'> [ 1 24 24 32]\n 58                conv2d_6/Kernel <class 'numpy.float32'> [64  1  1 32]\n 59                  conv2d_6/Bias <class 'numpy.float32'> [64]\n 60                       conv2d_6 <class 'numpy.float32'> [ 1 24 24 64]\n 61     channel_padding_1/Paddings <class 'numpy.int32'> [4 2]\n 62              channel_padding_1 <class 'numpy.float32'> [ 1 24 24 64]\n 63                          add_5 <class 'numpy.float32'> [ 1 24 24 64]\n 64                p_re_lu_6/Alpha <class 'numpy.float32'> [ 1  1 64]\n 65                      p_re_lu_6 <class 'numpy.float32'> [ 1 24 24 64]\n 66      depthwise_conv2d_6/Kernel <class 'numpy.float32'> [ 1  3  3 64]\n 67        depthwise_conv2d_6/Bias <class 'numpy.float32'> [64]\n 68             depthwise_conv2d_6 <class 'numpy.float32'> [ 1 24 24 64]\n 69                conv2d_7/Kernel <class 'numpy.float32'> [64  1  1 64]\n 70                  conv2d_7/Bias <class 'numpy.float32'> [64]\n 71                       conv2d_7 <class 'numpy.float32'> [ 1 24 24 64]\n 72                          add_6 <class 'numpy.float32'> [ 1 24 24 64]\n 73                p_re_lu_7/Alpha <class 'numpy.float32'> [ 1  1 64]\n 74                      p_re_lu_7 <class 'numpy.float32'> [ 1 24 24 64]\n 75      depthwise_conv2d_7/Kernel <class 'numpy.float32'> [ 1  3  3 64]\n 76        depthwise_conv2d_7/Bias <class 'numpy.float32'> [64]\n 77             depthwise_conv2d_7 <class 'numpy.float32'> [ 1 24 24 64]\n 78                conv2d_8/Kernel <class 'numpy.float32'> [64  1  1 64]\n 79                  conv2d_8/Bias <class 'numpy.float32'> [64]\n 80                       conv2d_8 <class 'numpy.float32'> [ 1 24 24 64]\n 81                          add_7 <class 'numpy.float32'> [ 1 24 24 64]\n 82                p_re_lu_8/Alpha <class 'numpy.float32'> [ 1  1 64]\n 83                      p_re_lu_8 <class 'numpy.float32'> [ 1 24 24 64]\n 84      depthwise_conv2d_8/Kernel <class 'numpy.float32'> [ 1  3  3 64]\n 85        depthwise_conv2d_8/Bias <class 'numpy.float32'> [64]\n 86             depthwise_conv2d_8 <class 'numpy.float32'> [ 1 12 12 64]\n 87                max_pooling2d_2 <class 'numpy.float32'> [ 1 12 12 64]\n 88                conv2d_9/Kernel <class 'numpy.float32'> [128   1   1  64]\n 89                  conv2d_9/Bias <class 'numpy.float32'> [128]\n 90                       conv2d_9 <class 'numpy.float32'> [  1  12  12 128]\n 91     channel_padding_2/Paddings <class 'numpy.int32'> [4 2]\n 92              channel_padding_2 <class 'numpy.float32'> [  1  12  12 128]\n 93                          add_8 <class 'numpy.float32'> [  1  12  12 128]\n 94                p_re_lu_9/Alpha <class 'numpy.float32'> [  1   1 128]\n 95                      p_re_lu_9 <class 'numpy.float32'> [  1  12  12 128]\n 96      depthwise_conv2d_9/Kernel <class 'numpy.float32'> [  1   3   3 128]\n 97        depthwise_conv2d_9/Bias <class 'numpy.float32'> [128]\n 98             depthwise_conv2d_9 <class 'numpy.float32'> [  1  12  12 128]\n 99               conv2d_10/Kernel <class 'numpy.float32'> [128   1   1 128]\n100                 conv2d_10/Bias <class 'numpy.float32'> [128]\n101                      conv2d_10 <class 'numpy.float32'> [  1  12  12 128]\n102                          add_9 <class 'numpy.float32'> [  1  12  12 128]\n103               p_re_lu_10/Alpha <class 'numpy.float32'> [  1   1 128]\n104                     p_re_lu_10 <class 'numpy.float32'> [  1  12  12 128]\n105     depthwise_conv2d_10/Kernel <class 'numpy.float32'> [  1   3   3 128]\n106       depthwise_conv2d_10/Bias <class 'numpy.float32'> [128]\n107            depthwise_conv2d_10 <class 'numpy.float32'> [  1  12  12 128]\n108               conv2d_11/Kernel <class 'numpy.float32'> [128   1   1 128]\n109                 conv2d_11/Bias <class 'numpy.float32'> [128]\n110                      conv2d_11 <class 'numpy.float32'> [  1  12  12 128]\n111                         add_10 <class 'numpy.float32'> [  1  12  12 128]\n112               p_re_lu_11/Alpha <class 'numpy.float32'> [  1   1 128]\n113                     p_re_lu_11 <class 'numpy.float32'> [  1  12  12 128]\n114     depthwise_conv2d_11/Kernel <class 'numpy.float32'> [  1   3   3 128]\n115       depthwise_conv2d_11/Bias <class 'numpy.float32'> [128]\n116            depthwise_conv2d_11 <class 'numpy.float32'> [  1   6   6 128]\n117               conv2d_12/Kernel <class 'numpy.float32'> [128   1   1 128]\n118                 conv2d_12/Bias <class 'numpy.float32'> [128]\n119                      conv2d_12 <class 'numpy.float32'> [  1   6   6 128]\n120                max_pooling2d_3 <class 'numpy.float32'> [  1   6   6 128]\n121                         add_11 <class 'numpy.float32'> [  1   6   6 128]\n122               p_re_lu_12/Alpha <class 'numpy.float32'> [  1   1 128]\n123                     p_re_lu_12 <class 'numpy.float32'> [  1   6   6 128]\n124     depthwise_conv2d_12/Kernel <class 'numpy.float32'> [  1   3   3 128]\n125       depthwise_conv2d_12/Bias <class 'numpy.float32'> [128]\n126            depthwise_conv2d_12 <class 'numpy.float32'> [  1   6   6 128]\n127               conv2d_13/Kernel <class 'numpy.float32'> [128   1   1 128]\n128                 conv2d_13/Bias <class 'numpy.float32'> [128]\n129                      conv2d_13 <class 'numpy.float32'> [  1   6   6 128]\n130                         add_12 <class 'numpy.float32'> [  1   6   6 128]\n131               p_re_lu_13/Alpha <class 'numpy.float32'> [  1   1 128]\n132                     p_re_lu_13 <class 'numpy.float32'> [  1   6   6 128]\n133     depthwise_conv2d_13/Kernel <class 'numpy.float32'> [  1   3   3 128]\n134       depthwise_conv2d_13/Bias <class 'numpy.float32'> [128]\n135            depthwise_conv2d_13 <class 'numpy.float32'> [  1   6   6 128]\n136               conv2d_14/Kernel <class 'numpy.float32'> [128   1   1 128]\n137                 conv2d_14/Bias <class 'numpy.float32'> [128]\n138                      conv2d_14 <class 'numpy.float32'> [  1   6   6 128]\n139                         add_13 <class 'numpy.float32'> [  1   6   6 128]\n140               p_re_lu_14/Alpha <class 'numpy.float32'> [  1   1 128]\n141                     p_re_lu_14 <class 'numpy.float32'> [  1   6   6 128]\n142     depthwise_conv2d_14/Kernel <class 'numpy.float32'> [  1   3   3 128]\n143       depthwise_conv2d_14/Bias <class 'numpy.float32'> [128]\n144            depthwise_conv2d_14 <class 'numpy.float32'> [  1   3   3 128]\n145               conv2d_15/Kernel <class 'numpy.float32'> [128   1   1 128]\n146                 conv2d_15/Bias <class 'numpy.float32'> [128]\n147                      conv2d_15 <class 'numpy.float32'> [  1   3   3 128]\n148                max_pooling2d_4 <class 'numpy.float32'> [  1   3   3 128]\n149                         add_14 <class 'numpy.float32'> [  1   3   3 128]\n150               p_re_lu_15/Alpha <class 'numpy.float32'> [  1   1 128]\n151                     p_re_lu_15 <class 'numpy.float32'> [  1   3   3 128]\n152     depthwise_conv2d_15/Kernel <class 'numpy.float32'> [  1   3   3 128]\n153       depthwise_conv2d_15/Bias <class 'numpy.float32'> [128]\n154            depthwise_conv2d_15 <class 'numpy.float32'> [  1   3   3 128]\n155               conv2d_16/Kernel <class 'numpy.float32'> [128   1   1 128]\n156                 conv2d_16/Bias <class 'numpy.float32'> [128]\n157                      conv2d_16 <class 'numpy.float32'> [  1   3   3 128]\n158                         add_15 <class 'numpy.float32'> [  1   3   3 128]\n159               p_re_lu_16/Alpha <class 'numpy.float32'> [  1   1 128]\n160                     p_re_lu_16 <class 'numpy.float32'> [  1   3   3 128]\n161     depthwise_conv2d_22/Kernel <class 'numpy.float32'> [  1   3   3 128]\n162       depthwise_conv2d_22/Bias <class 'numpy.float32'> [128]\n163            depthwise_conv2d_22 <class 'numpy.float32'> [  1   3   3 128]\n164     depthwise_conv2d_16/Kernel <class 'numpy.float32'> [  1   3   3 128]\n165       depthwise_conv2d_16/Bias <class 'numpy.float32'> [128]\n166            depthwise_conv2d_16 <class 'numpy.float32'> [  1   3   3 128]\n167               conv2d_27/Kernel <class 'numpy.float32'> [128   1   1 128]\n168                 conv2d_27/Bias <class 'numpy.float32'> [128]\n169                      conv2d_27 <class 'numpy.float32'> [  1   3   3 128]\n170               conv2d_17/Kernel <class 'numpy.float32'> [128   1   1 128]\n171                 conv2d_17/Bias <class 'numpy.float32'> [128]\n172                      conv2d_17 <class 'numpy.float32'> [  1   3   3 128]\n173                max_pooling2d_6 <class 'numpy.float32'> [  1   3   3 128]\n174                         add_22 <class 'numpy.float32'> [  1   3   3 128]\n175                         add_16 <class 'numpy.float32'> [  1   3   3 128]\n176               p_re_lu_25/Alpha <class 'numpy.float32'> [  1   1 128]\n177                     p_re_lu_25 <class 'numpy.float32'> [  1   3   3 128]\n178               p_re_lu_17/Alpha <class 'numpy.float32'> [  1   1 128]\n179                     p_re_lu_17 <class 'numpy.float32'> [  1   3   3 128]\n180               conv2d_28/Kernel <class 'numpy.float32'> [ 32   1   1 128]\n181                 conv2d_28/Bias <class 'numpy.float32'> [32]\n182                      conv2d_28 <class 'numpy.float32'> [ 1  3  3 32]\n183               conv2d_18/Kernel <class 'numpy.float32'> [ 32   1   1 128]\n184                 conv2d_18/Bias <class 'numpy.float32'> [32]\n185                      conv2d_18 <class 'numpy.float32'> [ 1  3  3 32]\n186               p_re_lu_26/Alpha <class 'numpy.float32'> [ 1  1 32]\n187                     p_re_lu_26 <class 'numpy.float32'> [ 1  3  3 32]\n188               p_re_lu_18/Alpha <class 'numpy.float32'> [ 1  1 32]\n189                     p_re_lu_18 <class 'numpy.float32'> [ 1  3  3 32]\n190     depthwise_conv2d_23/Kernel <class 'numpy.float32'> [ 1  3  3 32]\n191       depthwise_conv2d_23/Bias <class 'numpy.float32'> [32]\n192            depthwise_conv2d_23 <class 'numpy.float32'> [ 1  3  3 32]\n193     depthwise_conv2d_17/Kernel <class 'numpy.float32'> [ 1  3  3 32]\n194       depthwise_conv2d_17/Bias <class 'numpy.float32'> [32]\n195            depthwise_conv2d_17 <class 'numpy.float32'> [ 1  3  3 32]\n196               conv2d_29/Kernel <class 'numpy.float32'> [32  1  1 32]\n197                 conv2d_29/Bias <class 'numpy.float32'> [32]\n198                      conv2d_29 <class 'numpy.float32'> [ 1  3  3 32]\n199               conv2d_19/Kernel <class 'numpy.float32'> [32  1  1 32]\n200                 conv2d_19/Bias <class 'numpy.float32'> [32]\n201                      conv2d_19 <class 'numpy.float32'> [ 1  3  3 32]\n202                         add_23 <class 'numpy.float32'> [ 1  3  3 32]\n203                         add_17 <class 'numpy.float32'> [ 1  3  3 32]\n204               p_re_lu_27/Alpha <class 'numpy.float32'> [ 1  1 32]\n205                     p_re_lu_27 <class 'numpy.float32'> [ 1  3  3 32]\n206               p_re_lu_19/Alpha <class 'numpy.float32'> [ 1  1 32]\n207                     p_re_lu_19 <class 'numpy.float32'> [ 1  3  3 32]\n208               conv2d_30/Kernel <class 'numpy.float32'> [ 1  3  3 32]\n209                 conv2d_30/Bias <class 'numpy.float32'> [1]\n210                      conv2d_30 <class 'numpy.float32'> [1 1 1 1]\n211               conv2d_20/Kernel <class 'numpy.float32'> [1404    3    3   32]\n212                 conv2d_20/Bias <class 'numpy.float32'> [1404]\n213                      conv2d_20 <class 'numpy.float32'> [   1    1    1 1404]\n"
     ]
    }
   ],
   "source": [
    "# モデルの情報を確認\n",
    "for d in interpreter.get_tensor_details():\n",
    "    print(\"%3d %30s %15s %s\" % (d['index'], d['name'], d['dtype'], d['shape']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テンソル名に基づいてテンソルインデックスを取得できるようなルックアップテーブルを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_1': 0,\n",
       " 'conv2d/Kernel': 1,\n",
       " 'conv2d/Bias': 2,\n",
       " 'conv2d': 3,\n",
       " 'p_re_lu/Alpha': 4,\n",
       " 'p_re_lu': 5,\n",
       " 'depthwise_conv2d/Kernel': 6,\n",
       " 'depthwise_conv2d/Bias': 7,\n",
       " 'depthwise_conv2d': 8,\n",
       " 'conv2d_1/Kernel': 9,\n",
       " 'conv2d_1/Bias': 10,\n",
       " 'conv2d_1': 11,\n",
       " 'add': 12,\n",
       " 'p_re_lu_1/Alpha': 13,\n",
       " 'p_re_lu_1': 14,\n",
       " 'depthwise_conv2d_1/Kernel': 15,\n",
       " 'depthwise_conv2d_1/Bias': 16,\n",
       " 'depthwise_conv2d_1': 17,\n",
       " 'conv2d_2/Kernel': 18,\n",
       " 'conv2d_2/Bias': 19,\n",
       " 'conv2d_2': 20,\n",
       " 'add_1': 21,\n",
       " 'p_re_lu_2/Alpha': 22,\n",
       " 'p_re_lu_2': 23,\n",
       " 'depthwise_conv2d_2/Kernel': 24,\n",
       " 'depthwise_conv2d_2/Bias': 25,\n",
       " 'depthwise_conv2d_2': 26,\n",
       " 'max_pooling2d': 27,\n",
       " 'conv2d_3/Kernel': 28,\n",
       " 'conv2d_3/Bias': 29,\n",
       " 'conv2d_3': 30,\n",
       " 'channel_padding/Paddings': 31,\n",
       " 'channel_padding': 32,\n",
       " 'add_2': 33,\n",
       " 'p_re_lu_3/Alpha': 34,\n",
       " 'p_re_lu_3': 35,\n",
       " 'depthwise_conv2d_3/Kernel': 36,\n",
       " 'depthwise_conv2d_3/Bias': 37,\n",
       " 'depthwise_conv2d_3': 38,\n",
       " 'conv2d_4/Kernel': 39,\n",
       " 'conv2d_4/Bias': 40,\n",
       " 'conv2d_4': 41,\n",
       " 'add_3': 42,\n",
       " 'p_re_lu_4/Alpha': 43,\n",
       " 'p_re_lu_4': 44,\n",
       " 'depthwise_conv2d_4/Kernel': 45,\n",
       " 'depthwise_conv2d_4/Bias': 46,\n",
       " 'depthwise_conv2d_4': 47,\n",
       " 'conv2d_5/Kernel': 48,\n",
       " 'conv2d_5/Bias': 49,\n",
       " 'conv2d_5': 50,\n",
       " 'add_4': 51,\n",
       " 'p_re_lu_5/Alpha': 52,\n",
       " 'p_re_lu_5': 53,\n",
       " 'depthwise_conv2d_5/Kernel': 54,\n",
       " 'depthwise_conv2d_5/Bias': 55,\n",
       " 'depthwise_conv2d_5': 56,\n",
       " 'max_pooling2d_1': 57,\n",
       " 'conv2d_6/Kernel': 58,\n",
       " 'conv2d_6/Bias': 59,\n",
       " 'conv2d_6': 60,\n",
       " 'channel_padding_1/Paddings': 61,\n",
       " 'channel_padding_1': 62,\n",
       " 'add_5': 63,\n",
       " 'p_re_lu_6/Alpha': 64,\n",
       " 'p_re_lu_6': 65,\n",
       " 'depthwise_conv2d_6/Kernel': 66,\n",
       " 'depthwise_conv2d_6/Bias': 67,\n",
       " 'depthwise_conv2d_6': 68,\n",
       " 'conv2d_7/Kernel': 69,\n",
       " 'conv2d_7/Bias': 70,\n",
       " 'conv2d_7': 71,\n",
       " 'add_6': 72,\n",
       " 'p_re_lu_7/Alpha': 73,\n",
       " 'p_re_lu_7': 74,\n",
       " 'depthwise_conv2d_7/Kernel': 75,\n",
       " 'depthwise_conv2d_7/Bias': 76,\n",
       " 'depthwise_conv2d_7': 77,\n",
       " 'conv2d_8/Kernel': 78,\n",
       " 'conv2d_8/Bias': 79,\n",
       " 'conv2d_8': 80,\n",
       " 'add_7': 81,\n",
       " 'p_re_lu_8/Alpha': 82,\n",
       " 'p_re_lu_8': 83,\n",
       " 'depthwise_conv2d_8/Kernel': 84,\n",
       " 'depthwise_conv2d_8/Bias': 85,\n",
       " 'depthwise_conv2d_8': 86,\n",
       " 'max_pooling2d_2': 87,\n",
       " 'conv2d_9/Kernel': 88,\n",
       " 'conv2d_9/Bias': 89,\n",
       " 'conv2d_9': 90,\n",
       " 'channel_padding_2/Paddings': 91,\n",
       " 'channel_padding_2': 92,\n",
       " 'add_8': 93,\n",
       " 'p_re_lu_9/Alpha': 94,\n",
       " 'p_re_lu_9': 95,\n",
       " 'depthwise_conv2d_9/Kernel': 96,\n",
       " 'depthwise_conv2d_9/Bias': 97,\n",
       " 'depthwise_conv2d_9': 98,\n",
       " 'conv2d_10/Kernel': 99,\n",
       " 'conv2d_10/Bias': 100,\n",
       " 'conv2d_10': 101,\n",
       " 'add_9': 102,\n",
       " 'p_re_lu_10/Alpha': 103,\n",
       " 'p_re_lu_10': 104,\n",
       " 'depthwise_conv2d_10/Kernel': 105,\n",
       " 'depthwise_conv2d_10/Bias': 106,\n",
       " 'depthwise_conv2d_10': 107,\n",
       " 'conv2d_11/Kernel': 108,\n",
       " 'conv2d_11/Bias': 109,\n",
       " 'conv2d_11': 110,\n",
       " 'add_10': 111,\n",
       " 'p_re_lu_11/Alpha': 112,\n",
       " 'p_re_lu_11': 113,\n",
       " 'depthwise_conv2d_11/Kernel': 114,\n",
       " 'depthwise_conv2d_11/Bias': 115,\n",
       " 'depthwise_conv2d_11': 116,\n",
       " 'conv2d_12/Kernel': 117,\n",
       " 'conv2d_12/Bias': 118,\n",
       " 'conv2d_12': 119,\n",
       " 'max_pooling2d_3': 120,\n",
       " 'add_11': 121,\n",
       " 'p_re_lu_12/Alpha': 122,\n",
       " 'p_re_lu_12': 123,\n",
       " 'depthwise_conv2d_12/Kernel': 124,\n",
       " 'depthwise_conv2d_12/Bias': 125,\n",
       " 'depthwise_conv2d_12': 126,\n",
       " 'conv2d_13/Kernel': 127,\n",
       " 'conv2d_13/Bias': 128,\n",
       " 'conv2d_13': 129,\n",
       " 'add_12': 130,\n",
       " 'p_re_lu_13/Alpha': 131,\n",
       " 'p_re_lu_13': 132,\n",
       " 'depthwise_conv2d_13/Kernel': 133,\n",
       " 'depthwise_conv2d_13/Bias': 134,\n",
       " 'depthwise_conv2d_13': 135,\n",
       " 'conv2d_14/Kernel': 136,\n",
       " 'conv2d_14/Bias': 137,\n",
       " 'conv2d_14': 138,\n",
       " 'add_13': 139,\n",
       " 'p_re_lu_14/Alpha': 140,\n",
       " 'p_re_lu_14': 141,\n",
       " 'depthwise_conv2d_14/Kernel': 142,\n",
       " 'depthwise_conv2d_14/Bias': 143,\n",
       " 'depthwise_conv2d_14': 144,\n",
       " 'conv2d_15/Kernel': 145,\n",
       " 'conv2d_15/Bias': 146,\n",
       " 'conv2d_15': 147,\n",
       " 'max_pooling2d_4': 148,\n",
       " 'add_14': 149,\n",
       " 'p_re_lu_15/Alpha': 150,\n",
       " 'p_re_lu_15': 151,\n",
       " 'depthwise_conv2d_15/Kernel': 152,\n",
       " 'depthwise_conv2d_15/Bias': 153,\n",
       " 'depthwise_conv2d_15': 154,\n",
       " 'conv2d_16/Kernel': 155,\n",
       " 'conv2d_16/Bias': 156,\n",
       " 'conv2d_16': 157,\n",
       " 'add_15': 158,\n",
       " 'p_re_lu_16/Alpha': 159,\n",
       " 'p_re_lu_16': 160,\n",
       " 'depthwise_conv2d_22/Kernel': 161,\n",
       " 'depthwise_conv2d_22/Bias': 162,\n",
       " 'depthwise_conv2d_22': 163,\n",
       " 'depthwise_conv2d_16/Kernel': 164,\n",
       " 'depthwise_conv2d_16/Bias': 165,\n",
       " 'depthwise_conv2d_16': 166,\n",
       " 'conv2d_27/Kernel': 167,\n",
       " 'conv2d_27/Bias': 168,\n",
       " 'conv2d_27': 169,\n",
       " 'conv2d_17/Kernel': 170,\n",
       " 'conv2d_17/Bias': 171,\n",
       " 'conv2d_17': 172,\n",
       " 'max_pooling2d_6': 173,\n",
       " 'add_22': 174,\n",
       " 'add_16': 175,\n",
       " 'p_re_lu_25/Alpha': 176,\n",
       " 'p_re_lu_25': 177,\n",
       " 'p_re_lu_17/Alpha': 178,\n",
       " 'p_re_lu_17': 179,\n",
       " 'conv2d_28/Kernel': 180,\n",
       " 'conv2d_28/Bias': 181,\n",
       " 'conv2d_28': 182,\n",
       " 'conv2d_18/Kernel': 183,\n",
       " 'conv2d_18/Bias': 184,\n",
       " 'conv2d_18': 185,\n",
       " 'p_re_lu_26/Alpha': 186,\n",
       " 'p_re_lu_26': 187,\n",
       " 'p_re_lu_18/Alpha': 188,\n",
       " 'p_re_lu_18': 189,\n",
       " 'depthwise_conv2d_23/Kernel': 190,\n",
       " 'depthwise_conv2d_23/Bias': 191,\n",
       " 'depthwise_conv2d_23': 192,\n",
       " 'depthwise_conv2d_17/Kernel': 193,\n",
       " 'depthwise_conv2d_17/Bias': 194,\n",
       " 'depthwise_conv2d_17': 195,\n",
       " 'conv2d_29/Kernel': 196,\n",
       " 'conv2d_29/Bias': 197,\n",
       " 'conv2d_29': 198,\n",
       " 'conv2d_19/Kernel': 199,\n",
       " 'conv2d_19/Bias': 200,\n",
       " 'conv2d_19': 201,\n",
       " 'add_23': 202,\n",
       " 'add_17': 203,\n",
       " 'p_re_lu_27/Alpha': 204,\n",
       " 'p_re_lu_27': 205,\n",
       " 'p_re_lu_19/Alpha': 206,\n",
       " 'p_re_lu_19': 207,\n",
       " 'conv2d_30/Kernel': 208,\n",
       " 'conv2d_30/Bias': 209,\n",
       " 'conv2d_30': 210,\n",
       " 'conv2d_20/Kernel': 211,\n",
       " 'conv2d_20/Bias': 212,\n",
       " 'conv2d_20': 213}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "tensor_dict = {d['name']: d['index'] for d in interpreter.get_tensor_details()}\n",
    "tensor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_with_weight(tensor, info):\n",
    "    if ('add' in info['name']) or ('padding' in info['name']):\n",
    "        return False\n",
    "    elif tensor.sum()==0:\n",
    "        if not '/' in info['name']:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  0                  conv2d/Kernel   1\n  1                    conv2d/Bias   2\n  2                  p_re_lu/Alpha   4\n  3        depthwise_conv2d/Kernel   6\n  4          depthwise_conv2d/Bias   7\n  5                conv2d_1/Kernel   9\n  6                  conv2d_1/Bias  10\n  7                p_re_lu_1/Alpha  13\n  8      depthwise_conv2d_1/Kernel  15\n  9        depthwise_conv2d_1/Bias  16\n 10                conv2d_2/Kernel  18\n 11                  conv2d_2/Bias  19\n 12                p_re_lu_2/Alpha  22\n 13      depthwise_conv2d_2/Kernel  24\n 14        depthwise_conv2d_2/Bias  25\n 15                conv2d_3/Kernel  28\n 16                  conv2d_3/Bias  29\n 17                p_re_lu_3/Alpha  34\n 18      depthwise_conv2d_3/Kernel  36\n 19        depthwise_conv2d_3/Bias  37\n 20                conv2d_4/Kernel  39\n 21                  conv2d_4/Bias  40\n 22                p_re_lu_4/Alpha  43\n 23      depthwise_conv2d_4/Kernel  45\n 24        depthwise_conv2d_4/Bias  46\n 25                conv2d_5/Kernel  48\n 26                  conv2d_5/Bias  49\n 27                p_re_lu_5/Alpha  52\n 28      depthwise_conv2d_5/Kernel  54\n 29        depthwise_conv2d_5/Bias  55\n 30                conv2d_6/Kernel  58\n 31                  conv2d_6/Bias  59\n 32                p_re_lu_6/Alpha  64\n 33      depthwise_conv2d_6/Kernel  66\n 34        depthwise_conv2d_6/Bias  67\n 35                conv2d_7/Kernel  69\n 36                  conv2d_7/Bias  70\n 37                p_re_lu_7/Alpha  73\n 38      depthwise_conv2d_7/Kernel  75\n 39        depthwise_conv2d_7/Bias  76\n 40                conv2d_8/Kernel  78\n 41                  conv2d_8/Bias  79\n 42                p_re_lu_8/Alpha  82\n 43      depthwise_conv2d_8/Kernel  84\n 44        depthwise_conv2d_8/Bias  85\n 45                conv2d_9/Kernel  88\n 46                  conv2d_9/Bias  89\n 47                p_re_lu_9/Alpha  94\n 48      depthwise_conv2d_9/Kernel  96\n 49        depthwise_conv2d_9/Bias  97\n 50               conv2d_10/Kernel  99\n 51                 conv2d_10/Bias 100\n 52               p_re_lu_10/Alpha 103\n 53     depthwise_conv2d_10/Kernel 105\n 54       depthwise_conv2d_10/Bias 106\n 55               conv2d_11/Kernel 108\n 56                 conv2d_11/Bias 109\n 57               p_re_lu_11/Alpha 112\n 58     depthwise_conv2d_11/Kernel 114\n 59       depthwise_conv2d_11/Bias 115\n 60               conv2d_12/Kernel 117\n 61                 conv2d_12/Bias 118\n 62               p_re_lu_12/Alpha 122\n 63     depthwise_conv2d_12/Kernel 124\n 64       depthwise_conv2d_12/Bias 125\n 65               conv2d_13/Kernel 127\n 66                 conv2d_13/Bias 128\n 67               p_re_lu_13/Alpha 131\n 68     depthwise_conv2d_13/Kernel 133\n 69       depthwise_conv2d_13/Bias 134\n 70               conv2d_14/Kernel 136\n 71                 conv2d_14/Bias 137\n 72               p_re_lu_14/Alpha 140\n 73     depthwise_conv2d_14/Kernel 142\n 74       depthwise_conv2d_14/Bias 143\n 75               conv2d_15/Kernel 145\n 76                 conv2d_15/Bias 146\n 77               p_re_lu_15/Alpha 150\n 78     depthwise_conv2d_15/Kernel 152\n 79       depthwise_conv2d_15/Bias 153\n 80               conv2d_16/Kernel 155\n 81                 conv2d_16/Bias 156\n 82               p_re_lu_16/Alpha 159\n 83     depthwise_conv2d_22/Kernel 161\n 84       depthwise_conv2d_22/Bias 162\n 85     depthwise_conv2d_16/Kernel 164\n 86       depthwise_conv2d_16/Bias 165\n 87               conv2d_27/Kernel 167\n 88                 conv2d_27/Bias 168\n 89               conv2d_17/Kernel 170\n 90                 conv2d_17/Bias 171\n 91               p_re_lu_25/Alpha 176\n 92               p_re_lu_17/Alpha 178\n 93               conv2d_28/Kernel 180\n 94                 conv2d_28/Bias 181\n 95               conv2d_18/Kernel 183\n 96                 conv2d_18/Bias 184\n 97               p_re_lu_26/Alpha 186\n 98               p_re_lu_18/Alpha 188\n 99     depthwise_conv2d_23/Kernel 190\n100       depthwise_conv2d_23/Bias 191\n101     depthwise_conv2d_17/Kernel 193\n102       depthwise_conv2d_17/Bias 194\n103               conv2d_29/Kernel 196\n104                 conv2d_29/Bias 197\n105               conv2d_19/Kernel 199\n106                 conv2d_19/Bias 200\n107               p_re_lu_27/Alpha 204\n108               p_re_lu_19/Alpha 206\n109               conv2d_30/Kernel 208\n110                 conv2d_30/Bias 209\n111               conv2d_20/Kernel 211\n112                 conv2d_20/Bias 212\n"
     ]
    }
   ],
   "source": [
    "tensors_info = interpreter.get_tensor_details()\n",
    "\n",
    "tflite_names=list() #tflite側の名前のリスト\n",
    "i=0\n",
    "for info in tensors_info:\n",
    "    tensor = interpreter.get_tensor(info['index'])\n",
    "    if is_with_weight(tensor, info):\n",
    "        tflite_names.append(info['name'])\n",
    "        print(\"%3d %30s %3d\" % (i, info['name'], info['index']))\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights(name):\n",
    "    idx = tensor_dict[name]\n",
    "    W = interpreter.get_tensor(idx)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(32, 1, 1, 128)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "W = get_weights('conv2d_18/Kernel')\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorchのフォーマットを確認する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from facemesh_pytorch import FaceMesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['backbone.0.weight',\n",
       " 'backbone.0.bias',\n",
       " 'backbone.1.weight',\n",
       " 'backbone.2.convs.0.weight',\n",
       " 'backbone.2.convs.0.bias',\n",
       " 'backbone.2.convs.1.weight',\n",
       " 'backbone.2.convs.1.bias',\n",
       " 'backbone.2.act.weight',\n",
       " 'backbone.3.convs.0.weight',\n",
       " 'backbone.3.convs.0.bias']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "net = FaceMesh()\n",
    "pytorch_names = list(net.state_dict().keys())\n",
    "pytorch_names[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tflite側とpytorchの重みの数が一致することを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "113 113\n"
     ]
    }
   ],
   "source": [
    "print(len(pytorch_names), len(tflite_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2つのモデル間のレイヤー名をマッピングするルックアップテーブルを作成\n",
    "\n",
    "テンソルが両方のモデルで同じ順番であると仮定  \n",
    "分岐以降がぐちゃぐちゃなので、マニュアルで対応させる（力技）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert = {}\n",
    "for name_py, name_tf in zip(pytorch_names, tflite_names):\n",
    "    convert[name_py] = name_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2, 1404])\n"
     ]
    }
   ],
   "source": [
    "print(net(torch.randn(2,3,192,192))[0].shape)\n",
    "#torch.save(net.state_dict(), 'pytorch_model_tmp.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_mapping ={\n",
    " 'coord_head.0.convs.0.weight':'depthwise_conv2d_14/Kernel',\n",
    " 'coord_head.0.convs.0.bias':'depthwise_conv2d_14/Bias',\n",
    " 'coord_head.0.convs.1.weight':'conv2d_15/Kernel',\n",
    " 'coord_head.0.convs.1.bias':'conv2d_15/Bias',\n",
    " 'coord_head.0.act.weight':'p_re_lu_15/Alpha',\n",
    " 'coord_head.1.convs.0.weight':'depthwise_conv2d_15/Kernel',\n",
    " 'coord_head.1.convs.0.bias':'depthwise_conv2d_15/Bias',\n",
    " 'coord_head.1.convs.1.weight':'conv2d_16/Kernel',\n",
    " 'coord_head.1.convs.1.bias':'conv2d_16/Bias',\n",
    " 'coord_head.1.act.weight':'p_re_lu_16/Alpha',\n",
    " 'coord_head.2.convs.0.weight':'depthwise_conv2d_16/Kernel',\n",
    " 'coord_head.2.convs.0.bias':'depthwise_conv2d_16/Bias',\n",
    " 'coord_head.2.convs.1.weight':'conv2d_17/Kernel',\n",
    " 'coord_head.2.convs.1.bias':'conv2d_17/Bias',\n",
    " 'coord_head.2.act.weight':'p_re_lu_17/Alpha',\n",
    " 'coord_head.3.weight':'conv2d_18/Kernel',\n",
    " 'coord_head.3.bias':'conv2d_18/Bias',\n",
    " 'coord_head.4.weight':'p_re_lu_18/Alpha',\n",
    " 'coord_head.5.convs.0.weight':'depthwise_conv2d_17/Kernel',\n",
    " 'coord_head.5.convs.0.bias':'depthwise_conv2d_17/Bias',\n",
    " 'coord_head.5.convs.1.weight':'conv2d_19/Kernel',\n",
    " 'coord_head.5.convs.1.bias':'conv2d_19/Bias',\n",
    " 'coord_head.5.act.weight':'p_re_lu_19/Alpha',\n",
    " 'coord_head.6.weight':'conv2d_20/Kernel',\n",
    " 'coord_head.6.bias':'conv2d_20/Bias',\n",
    " 'conf_head.0.convs.0.weight':'depthwise_conv2d_22/Kernel',\n",
    " 'conf_head.0.convs.0.bias':'depthwise_conv2d_22/Bias',\n",
    " 'conf_head.0.convs.1.weight':'conv2d_27/Kernel',\n",
    " 'conf_head.0.convs.1.bias':'conv2d_27/Bias',\n",
    " 'conf_head.0.act.weight':'p_re_lu_25/Alpha',\n",
    " 'conf_head.1.weight':'conv2d_28/Kernel',\n",
    " 'conf_head.1.bias':'conv2d_28/Bias',\n",
    " 'conf_head.2.weight':'p_re_lu_26/Alpha',\n",
    " 'conf_head.3.convs.0.weight':'depthwise_conv2d_23/Kernel',\n",
    " 'conf_head.3.convs.0.bias':'depthwise_conv2d_23/Bias',\n",
    " 'conf_head.3.convs.1.weight':'conv2d_29/Kernel',\n",
    " 'conf_head.3.convs.1.bias':'conv2d_29/Bias',\n",
    " 'conf_head.3.act.weight':'p_re_lu_27/Alpha',\n",
    " 'conf_head.4.weight':'conv2d_30/Kernel',\n",
    " 'conf_head.4.bias':'conv2d_30/Bias'}\n",
    "convert.update(manual_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重みをレイヤーにコピー\n",
    "\n",
    "PyTorchとTFLiteでは重みの順序が異なるので、転置する必要がある\n",
    "\n",
    "Convolution weights:\n",
    "\n",
    "    TFLiteの場合 (out_channels, kernel_height, kernel_width, in_channels)\n",
    "    PyTorchの場合(out_channels, in_channels, kernel_height, kernel_width)\n",
    "\n",
    "Depthwise convolution weights\n",
    "\n",
    "    TFLiteの場合 (1, kernel_height, kernel_width, channels)\n",
    "    PyTorchの場合(channels, 1, kernel_height, kernel_width)\n",
    "    \n",
    "PReLU:\n",
    "\n",
    "    TFLiteの場合  (1, 1, 1, num_channels)\n",
    "    PyTorchの場合 (num_channels, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "backbone.0.weight conv2d/Kernel (16, 3, 3, 3) torch.Size([16, 3, 3, 3])\n",
      "backbone.0.bias conv2d/Bias (16,) torch.Size([16])\n",
      "backbone.1.weight p_re_lu/Alpha (1, 1, 16) torch.Size([16])\n",
      "backbone.2.convs.0.weight depthwise_conv2d/Kernel (1, 3, 3, 16) torch.Size([16, 1, 3, 3])\n",
      "backbone.2.convs.0.bias depthwise_conv2d/Bias (16,) torch.Size([16])\n",
      "backbone.2.convs.1.weight conv2d_1/Kernel (16, 1, 1, 16) torch.Size([16, 16, 1, 1])\n",
      "backbone.2.convs.1.bias conv2d_1/Bias (16,) torch.Size([16])\n",
      "backbone.2.act.weight p_re_lu_1/Alpha (1, 1, 16) torch.Size([16])\n",
      "backbone.3.convs.0.weight depthwise_conv2d_1/Kernel (1, 3, 3, 16) torch.Size([16, 1, 3, 3])\n",
      "backbone.3.convs.0.bias depthwise_conv2d_1/Bias (16,) torch.Size([16])\n",
      "backbone.3.convs.1.weight conv2d_2/Kernel (16, 1, 1, 16) torch.Size([16, 16, 1, 1])\n",
      "backbone.3.convs.1.bias conv2d_2/Bias (16,) torch.Size([16])\n",
      "backbone.3.act.weight p_re_lu_2/Alpha (1, 1, 16) torch.Size([16])\n",
      "backbone.4.convs.0.weight depthwise_conv2d_2/Kernel (1, 3, 3, 16) torch.Size([16, 1, 3, 3])\n",
      "backbone.4.convs.0.bias depthwise_conv2d_2/Bias (16,) torch.Size([16])\n",
      "backbone.4.convs.1.weight conv2d_3/Kernel (32, 1, 1, 16) torch.Size([32, 16, 1, 1])\n",
      "backbone.4.convs.1.bias conv2d_3/Bias (32,) torch.Size([32])\n",
      "backbone.4.act.weight p_re_lu_3/Alpha (1, 1, 32) torch.Size([32])\n",
      "backbone.5.convs.0.weight depthwise_conv2d_3/Kernel (1, 3, 3, 32) torch.Size([32, 1, 3, 3])\n",
      "backbone.5.convs.0.bias depthwise_conv2d_3/Bias (32,) torch.Size([32])\n",
      "backbone.5.convs.1.weight conv2d_4/Kernel (32, 1, 1, 32) torch.Size([32, 32, 1, 1])\n",
      "backbone.5.convs.1.bias conv2d_4/Bias (32,) torch.Size([32])\n",
      "backbone.5.act.weight p_re_lu_4/Alpha (1, 1, 32) torch.Size([32])\n",
      "backbone.6.convs.0.weight depthwise_conv2d_4/Kernel (1, 3, 3, 32) torch.Size([32, 1, 3, 3])\n",
      "backbone.6.convs.0.bias depthwise_conv2d_4/Bias (32,) torch.Size([32])\n",
      "backbone.6.convs.1.weight conv2d_5/Kernel (32, 1, 1, 32) torch.Size([32, 32, 1, 1])\n",
      "backbone.6.convs.1.bias conv2d_5/Bias (32,) torch.Size([32])\n",
      "backbone.6.act.weight p_re_lu_5/Alpha (1, 1, 32) torch.Size([32])\n",
      "backbone.7.convs.0.weight depthwise_conv2d_5/Kernel (1, 3, 3, 32) torch.Size([32, 1, 3, 3])\n",
      "backbone.7.convs.0.bias depthwise_conv2d_5/Bias (32,) torch.Size([32])\n",
      "backbone.7.convs.1.weight conv2d_6/Kernel (64, 1, 1, 32) torch.Size([64, 32, 1, 1])\n",
      "backbone.7.convs.1.bias conv2d_6/Bias (64,) torch.Size([64])\n",
      "backbone.7.act.weight p_re_lu_6/Alpha (1, 1, 64) torch.Size([64])\n",
      "backbone.8.convs.0.weight depthwise_conv2d_6/Kernel (1, 3, 3, 64) torch.Size([64, 1, 3, 3])\n",
      "backbone.8.convs.0.bias depthwise_conv2d_6/Bias (64,) torch.Size([64])\n",
      "backbone.8.convs.1.weight conv2d_7/Kernel (64, 1, 1, 64) torch.Size([64, 64, 1, 1])\n",
      "backbone.8.convs.1.bias conv2d_7/Bias (64,) torch.Size([64])\n",
      "backbone.8.act.weight p_re_lu_7/Alpha (1, 1, 64) torch.Size([64])\n",
      "backbone.9.convs.0.weight depthwise_conv2d_7/Kernel (1, 3, 3, 64) torch.Size([64, 1, 3, 3])\n",
      "backbone.9.convs.0.bias depthwise_conv2d_7/Bias (64,) torch.Size([64])\n",
      "backbone.9.convs.1.weight conv2d_8/Kernel (64, 1, 1, 64) torch.Size([64, 64, 1, 1])\n",
      "backbone.9.convs.1.bias conv2d_8/Bias (64,) torch.Size([64])\n",
      "backbone.9.act.weight p_re_lu_8/Alpha (1, 1, 64) torch.Size([64])\n",
      "backbone.10.convs.0.weight depthwise_conv2d_8/Kernel (1, 3, 3, 64) torch.Size([64, 1, 3, 3])\n",
      "backbone.10.convs.0.bias depthwise_conv2d_8/Bias (64,) torch.Size([64])\n",
      "backbone.10.convs.1.weight conv2d_9/Kernel (128, 1, 1, 64) torch.Size([128, 64, 1, 1])\n",
      "backbone.10.convs.1.bias conv2d_9/Bias (128,) torch.Size([128])\n",
      "backbone.10.act.weight p_re_lu_9/Alpha (1, 1, 128) torch.Size([128])\n",
      "backbone.11.convs.0.weight depthwise_conv2d_9/Kernel (1, 3, 3, 128) torch.Size([128, 1, 3, 3])\n",
      "backbone.11.convs.0.bias depthwise_conv2d_9/Bias (128,) torch.Size([128])\n",
      "backbone.11.convs.1.weight conv2d_10/Kernel (128, 1, 1, 128) torch.Size([128, 128, 1, 1])\n",
      "backbone.11.convs.1.bias conv2d_10/Bias (128,) torch.Size([128])\n",
      "backbone.11.act.weight p_re_lu_10/Alpha (1, 1, 128) torch.Size([128])\n",
      "backbone.12.convs.0.weight depthwise_conv2d_10/Kernel (1, 3, 3, 128) torch.Size([128, 1, 3, 3])\n",
      "backbone.12.convs.0.bias depthwise_conv2d_10/Bias (128,) torch.Size([128])\n",
      "backbone.12.convs.1.weight conv2d_11/Kernel (128, 1, 1, 128) torch.Size([128, 128, 1, 1])\n",
      "backbone.12.convs.1.bias conv2d_11/Bias (128,) torch.Size([128])\n",
      "backbone.12.act.weight p_re_lu_11/Alpha (1, 1, 128) torch.Size([128])\n",
      "backbone.13.convs.0.weight depthwise_conv2d_11/Kernel (1, 3, 3, 128) torch.Size([128, 1, 3, 3])\n",
      "backbone.13.convs.0.bias depthwise_conv2d_11/Bias (128,) torch.Size([128])\n",
      "backbone.13.convs.1.weight conv2d_12/Kernel (128, 1, 1, 128) torch.Size([128, 128, 1, 1])\n",
      "backbone.13.convs.1.bias conv2d_12/Bias (128,) torch.Size([128])\n",
      "backbone.13.act.weight p_re_lu_12/Alpha (1, 1, 128) torch.Size([128])\n",
      "backbone.14.convs.0.weight depthwise_conv2d_12/Kernel (1, 3, 3, 128) torch.Size([128, 1, 3, 3])\n",
      "backbone.14.convs.0.bias depthwise_conv2d_12/Bias (128,) torch.Size([128])\n",
      "backbone.14.convs.1.weight conv2d_13/Kernel (128, 1, 1, 128) torch.Size([128, 128, 1, 1])\n",
      "backbone.14.convs.1.bias conv2d_13/Bias (128,) torch.Size([128])\n",
      "backbone.14.act.weight p_re_lu_13/Alpha (1, 1, 128) torch.Size([128])\n",
      "backbone.15.convs.0.weight depthwise_conv2d_13/Kernel (1, 3, 3, 128) torch.Size([128, 1, 3, 3])\n",
      "backbone.15.convs.0.bias depthwise_conv2d_13/Bias (128,) torch.Size([128])\n",
      "backbone.15.convs.1.weight conv2d_14/Kernel (128, 1, 1, 128) torch.Size([128, 128, 1, 1])\n",
      "backbone.15.convs.1.bias conv2d_14/Bias (128,) torch.Size([128])\n",
      "backbone.15.act.weight p_re_lu_14/Alpha (1, 1, 128) torch.Size([128])\n",
      "coord_head.0.convs.0.weight depthwise_conv2d_14/Kernel (1, 3, 3, 128) torch.Size([128, 1, 3, 3])\n",
      "coord_head.0.convs.0.bias depthwise_conv2d_14/Bias (128,) torch.Size([128])\n",
      "coord_head.0.convs.1.weight conv2d_15/Kernel (128, 1, 1, 128) torch.Size([128, 128, 1, 1])\n",
      "coord_head.0.convs.1.bias conv2d_15/Bias (128,) torch.Size([128])\n",
      "coord_head.0.act.weight p_re_lu_15/Alpha (1, 1, 128) torch.Size([128])\n",
      "coord_head.1.convs.0.weight depthwise_conv2d_15/Kernel (1, 3, 3, 128) torch.Size([128, 1, 3, 3])\n",
      "coord_head.1.convs.0.bias depthwise_conv2d_15/Bias (128,) torch.Size([128])\n",
      "coord_head.1.convs.1.weight conv2d_16/Kernel (128, 1, 1, 128) torch.Size([128, 128, 1, 1])\n",
      "coord_head.1.convs.1.bias conv2d_16/Bias (128,) torch.Size([128])\n",
      "coord_head.1.act.weight p_re_lu_16/Alpha (1, 1, 128) torch.Size([128])\n",
      "coord_head.2.convs.0.weight depthwise_conv2d_16/Kernel (1, 3, 3, 128) torch.Size([128, 1, 3, 3])\n",
      "coord_head.2.convs.0.bias depthwise_conv2d_16/Bias (128,) torch.Size([128])\n",
      "coord_head.2.convs.1.weight conv2d_17/Kernel (128, 1, 1, 128) torch.Size([128, 128, 1, 1])\n",
      "coord_head.2.convs.1.bias conv2d_17/Bias (128,) torch.Size([128])\n",
      "coord_head.2.act.weight p_re_lu_17/Alpha (1, 1, 128) torch.Size([128])\n",
      "coord_head.3.weight conv2d_18/Kernel (32, 1, 1, 128) torch.Size([32, 128, 1, 1])\n",
      "coord_head.3.bias conv2d_18/Bias (32,) torch.Size([32])\n",
      "coord_head.4.weight p_re_lu_18/Alpha (1, 1, 32) torch.Size([32])\n",
      "coord_head.5.convs.0.weight depthwise_conv2d_17/Kernel (1, 3, 3, 32) torch.Size([32, 1, 3, 3])\n",
      "coord_head.5.convs.0.bias depthwise_conv2d_17/Bias (32,) torch.Size([32])\n",
      "coord_head.5.convs.1.weight conv2d_19/Kernel (32, 1, 1, 32) torch.Size([32, 32, 1, 1])\n",
      "coord_head.5.convs.1.bias conv2d_19/Bias (32,) torch.Size([32])\n",
      "coord_head.5.act.weight p_re_lu_19/Alpha (1, 1, 32) torch.Size([32])\n",
      "coord_head.6.weight conv2d_20/Kernel (1404, 3, 3, 32) torch.Size([1404, 32, 3, 3])\n",
      "coord_head.6.bias conv2d_20/Bias (1404,) torch.Size([1404])\n",
      "conf_head.0.convs.0.weight depthwise_conv2d_22/Kernel (1, 3, 3, 128) torch.Size([128, 1, 3, 3])\n",
      "conf_head.0.convs.0.bias depthwise_conv2d_22/Bias (128,) torch.Size([128])\n",
      "conf_head.0.convs.1.weight conv2d_27/Kernel (128, 1, 1, 128) torch.Size([128, 128, 1, 1])\n",
      "conf_head.0.convs.1.bias conv2d_27/Bias (128,) torch.Size([128])\n",
      "conf_head.0.act.weight p_re_lu_25/Alpha (1, 1, 128) torch.Size([128])\n",
      "conf_head.1.weight conv2d_28/Kernel (32, 1, 1, 128) torch.Size([32, 128, 1, 1])\n",
      "conf_head.1.bias conv2d_28/Bias (32,) torch.Size([32])\n",
      "conf_head.2.weight p_re_lu_26/Alpha (1, 1, 32) torch.Size([32])\n",
      "conf_head.3.convs.0.weight depthwise_conv2d_23/Kernel (1, 3, 3, 32) torch.Size([32, 1, 3, 3])\n",
      "conf_head.3.convs.0.bias depthwise_conv2d_23/Bias (32,) torch.Size([32])\n",
      "conf_head.3.convs.1.weight conv2d_29/Kernel (32, 1, 1, 32) torch.Size([32, 32, 1, 1])\n",
      "conf_head.3.convs.1.bias conv2d_29/Bias (32,) torch.Size([32])\n",
      "conf_head.3.act.weight p_re_lu_27/Alpha (1, 1, 32) torch.Size([32])\n",
      "conf_head.4.weight conv2d_30/Kernel (1, 3, 3, 32) torch.Size([1, 32, 3, 3])\n",
      "conf_head.4.bias conv2d_30/Bias (1,) torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "new_state_dict = OrderedDict()\n",
    "\n",
    "for dst, src in convert.items():\n",
    "    W = get_weights(src)\n",
    "    print(dst, src, W.shape, net.state_dict()[dst].shape)\n",
    "\n",
    "    if W.ndim == 4:\n",
    "        if W.shape[0] == 1 and dst != \"conf_head.4.weight\":\n",
    "            W = W.transpose((3, 0, 1, 2))  # depthwise conv\n",
    "        else:\n",
    "            W = W.transpose((0, 3, 1, 2))  # regular conv\n",
    "    elif W.ndim == 3:\n",
    "        W = W.reshape(-1)\n",
    "    \n",
    "    new_state_dict[dst] = torch.from_numpy(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "net.load_state_dict(new_state_dict, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通常のpytorchモデルを保存\n",
    "torch.save(net.state_dict(), '../models/pytorch/FaceMesh.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = torch.rand(1,3,192,192)\n",
    "net.eval()\n",
    "traced_script_module = torch.jit.trace(net, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_script_module.save('../models/libtorch/FaceMesh.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}